
from kfp.v2 import compiler
from kfp.v2.dsl import pipeline, component, Output, Dataset
from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp, DataprocClusterCreateOp, DataprocClusterDeleteOp
from google_cloud_pipeline_components.v1.bigquery import BigQueryQueryJobOp

PROJECT_ID = 'your-project-id'
REGION = 'your-region'
BUCKET_NAME = 'your-gcs-bucket'
CLUSTER_NAME = 'vertex-dataproc-cluster'
PYSPARK_FILE_URI = 'gs://your-bucket/path-to-your-pyspark-job.py'

@pipeline(name="vertex-ai-dataproc-pipeline", description="Pipeline with BigQuery and Dataproc integration")
def dataproc_pipeline():
    # Step 1: Fetch data from BigQuery
    bigquery_query = BigQueryQueryJobOp(
        query="""
            SELECT * FROM `your-project-id.your-dataset.your-table`
            LIMIT 1000
        """,
        project=PROJECT_ID,
        location=REGION,
        output_table=f"{PROJECT_ID}.temp_dataset.temp_table",  # Temporary BQ table for staging
        write_disposition="WRITE_TRUNCATE"
    )
    
    # Step 2: Save BigQuery output to GCS
    bigquery_to_gcs = BigQueryQueryJobOp(
        query=f"""
            EXPORT DATA OPTIONS(
                uri='gs://{BUCKET_NAME}/bigquery_output/*.csv',
                format='CSV',
                overwrite=true
            ) AS
            SELECT * FROM `{PROJECT_ID}.temp_dataset.temp_table`
        """,
        project=PROJECT_ID,
        location=REGION
    ).after(bigquery_query)
    
    # Step 3: Create a Dataproc cluster
    create_cluster = DataprocClusterCreateOp(
        project=PROJECT_ID,
        region=REGION,
        cluster_name=CLUSTER_NAME,
        gcs_bucket=BUCKET_NAME,
        cluster_config={
            "master_config": {"num_instances": 1, "machine_type_uri": "n1-standard-4"},
            "worker_config": {"num_instances": 2, "machine_type_uri": "n1-standard-4"},
        }
    ).after(bigquery_to_gcs)
    
    # Step 4: Submit PySpark job to Dataproc
    pyspark_task = DataprocPySparkBatchOp(
        project=PROJECT_ID,
        location=REGION,
        batch_id="pyspark-job",
        main_python_file_uri=PYSPARK_FILE_URI,
        cluster_name=create_cluster.output["cluster_name"],
        args=[
            f"gs://{BUCKET_NAME}/bigquery_output/*.csv",  # Input data path
            f"gs://{BUCKET_NAME}/processed_output/"       # Output data path
        ]
    ).after(create_cluster)
    
    # Step 5: Delete the cluster after the job is complete
    delete_cluster = DataprocClusterDeleteOp(
        project=PROJECT_ID,
        region=REGION,
        cluster_name=create_cluster.output["cluster_name"]
    ).after(pyspark_task)

compiler.Compiler().compile(
    pipeline_func=dataproc_pipeline,
    package_path="vertex_ai_dataproc_with_bigquery_pipeline.json"
)


from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='your-region')

# Trigger the pipeline
job = aiplatform.PipelineJob(
    display_name="vertex-ai-dataproc-pipeline",
    template_path="vertex_ai_dataproc_with_bigquery_pipeline.json",
    pipeline_root="gs://your-gcs-bucket/pipeline-root/",
    parameter_values={}  # Add any parameters if needed
)

job.run()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize stopwords, lemmatizer, and stemmer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("TextProcessing") \
    .getOrCreate()

# Input and output paths
input_path = "gs://your-bucket/bigquery_output/*.csv"  # Input data fetched from BigQuery
output_path = "gs://your-bucket/processed_output/"    # Processed output path

# Load data
df = spark.read.csv(input_path, header=True, inferSchema=True)

# UDFs for text processing
def clean_html(text):
    """Remove HTML tags from text."""
    return BeautifulSoup(text, "html.parser").get_text()

def clean_urls(text):
    """Remove URLs from text."""
    return re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

def remove_stopwords(text):
    """Remove stopwords from text."""
    words = text.split()
    return " ".join([word for word in words if word.lower() not in stop_words])

def lemmatize(text):
    """Perform lemmatization on text."""
    words = text.split()
    return " ".join([lemmatizer.lemmatize(word) for word in words])

def stem(text):
    """Perform stemming on text."""
    words = text.split()
    return " ".join([stemmer.stem(word) for word in words])

def process_text(text):
    """Perform full text processing: HTML removal, URL removal, stopwords removal, lemmatization, and stemming."""
    if not text:
        return ""
    text = clean_html(text)
    text = clean_urls(text)
    text = remove_stopwords(text)
    text = lemmatize(text)
    text = stem(text)
    return text

# Register UDF
process_text_udf = udf(process_text, StringType())

# Apply text processing
processed_df = df.withColumn("processed_text", process_text_udf(col("text")))

# Save processed data
processed_df.write.csv(output_path, header=True, mode="overwrite")

# Stop Spark Session
spark.stop()



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import tensorflow as tf
import nltk

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize stopwords, lemmatizer, and stemmer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("TextProcessingTFRecords") \
    .getOrCreate()

# Input and output paths
input_path = "gs://your-bucket/tfrecords_data/*.tfrecord"  # TFRecords input path
output_path = "gs://your-bucket/processed_output/"        # Processed output path

# Function to parse TFRecords
def parse_example(serialized_example):
    """Parse a single TFRecord example."""
    feature_description = {
        "text": tf.io.FixedLenFeature([], tf.string),  # Text column in TFRecord
    }
    example = tf.io.parse_single_example(serialized_example, feature_description)
    return {"text": example["text"].numpy().decode("utf-8")}

# Load TFRecords as a Spark DataFrame
raw_rdd = spark.sparkContext.newAPIHadoopFile(
    input_path,
    "org.tensorflow.hadoop.io.TFRecordFileInputFormat",
    "org.apache.hadoop.io.BytesWritable",
    "org.apache.hadoop.io.NullWritable",
).map(lambda x: parse_example(x[0].copyBytes()))

df = spark.createDataFrame(raw_rdd)

# UDFs for text processing
def clean_html(text):
    """Remove HTML tags from text."""
    return BeautifulSoup(text, "html.parser").get_text()

def clean_urls(text):
    """Remove URLs from text."""
    return re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

def remove_stopwords(text):
    """Remove stopwords from text."""
    words = text.split()
    return " ".join([word for word in words if word.lower() not in stop_words])

def lemmatize(text):
    """Perform lemmatization on text."""
    words = text.split()
    return " ".join([lemmatizer.lemmatize(word) for word in words])

def stem(text):
    """Perform stemming on text."""
    words = text.split()
    return " ".join([stemmer.stem(word) for word in words])

def process_text(text):
    """Perform full text processing: HTML removal, URL removal, stopwords removal, lemmatization, and stemming."""
    if not text:
        return ""
    text = clean_html(text)
    text = clean_urls(text)
    text = remove_stopwords(text)
    text = lemmatize(text)
    text = stem(text)
    return text

# Register UDF
process_text_udf = udf(process_text, StringType())

# Apply text processing
processed_df = df.withColumn("processed_text", process_text_udf(col("text")))

# Save processed data
processed_df.write.csv(output_path, header=True, mode="overwrite")

# Stop Spark Session
spark.stop()


from google_cloud_pipeline_components.v1.dataproc.create_pyspark_batch.component import dataproc_create_pyspark_batch as DataprocPySparkBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_batch.component import dataproc_create_spark_batch as DataprocSparkBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_r_batch.component import dataproc_create_spark_r_batch as DataprocSparkRBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_sql_batch.component import dataproc_create_spark_sql_batch as DataprocSparkSqlBatchOp


from google_cloud_pipeline_components.v1.bigquery.create_model.component import bigquery_create_model_job as BigqueryCreateModelJobOp
from google_cloud_pipeline_components.v1.bigquery.detect_anomalies_model.component import bigquery_detect_anomalies_job as BigqueryDetectAnomaliesModelJobOp
from google_cloud_pipeline_components.v1.bigquery.drop_model.component import bigquery_drop_model_job as BigqueryDropModelJobOp
from google_cloud_pipeline_components.v1.bigquery.evaluate_model.component import bigquery_evaluate_model_job as BigqueryEvaluateModelJobOp
from google_cloud_pipeline_components.v1.bigquery.explain_forecast_model.component import bigquery_explain_forecast_model_job as BigqueryExplainForecastModelJobOp
from google_cloud_pipeline_components.v1.bigquery.explain_predict_model.component import bigquery_explain_predict_model_job as BigqueryExplainPredictModelJobOp
from google_cloud_pipeline_components.v1.bigquery.export_model.component import bigquery_export_model_job as BigqueryExportModelJobOp
from google_cloud_pipeline_components.v1.bigquery.feature_importance.component import bigquery_ml_feature_importance_job as BigqueryMLFeatureImportanceJobOp
from google_cloud_pipeline_components.v1.bigquery.forecast_model.component import bigquery_forecast_model_job as BigqueryForecastModelJobOp
from google_cloud_pipeline_components.v1.bigquery.global_explain.component import bigquery_ml_global_explain_job as BigqueryMLGlobalExplainJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_advanced_weights.component import bigquery_ml_advanced_weights_job as BigqueryMLAdvancedWeightsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_arima_coefficients.component import bigquery_ml_arima_coefficients as BigqueryMLArimaCoefficientsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_arima_evaluate.component import bigquery_ml_arima_evaluate_job as BigqueryMLArimaEvaluateJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_centroids.component import bigquery_ml_centroids_job as BigqueryMLCentroidsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_confusion_matrix.component import bigquery_ml_confusion_matrix_job as BigqueryMLConfusionMatrixJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_feature_info.component import bigquery_ml_feature_info_job as BigqueryMLFeatureInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_principal_component_info.component import bigquery_ml_principal_component_info_job as BigqueryMLPrincipalComponentInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_principal_components.component import bigquery_ml_principal_components_job as BigqueryMLPrincipalComponentsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_recommend.component import bigquery_ml_recommend_job as BigqueryMLRecommendJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_reconstruction_loss.component import bigquery_ml_reconstruction_loss_job as BigqueryMLReconstructionLossJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_roc_curve.component import bigquery_ml_roc_curve_job as BigqueryMLRocCurveJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_training_info.component import bigquery_ml_training_info_job as BigqueryMLTrainingInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_trial_info.component import bigquery_ml_trial_info_job as BigqueryMLTrialInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_weights.component import bigquery_ml_weights_job as BigqueryMLWeightsJobOp
from google_cloud_pipeline_components.v1.bigquery.predict_model.component import bigquery_predict_model_job as BigqueryPredictModelJobOp
from google_cloud_pipeline_components.v1.bigquery.query_job.component import bigquery_query_job as BigqueryQueryJobOp



from kfp.v2.dsl import pipeline
from google_cloud_pipeline_components.v1.dataproc import DataprocSparkSqlBatchOp, DataprocPySparkBatchOp

@pipeline(name="dataproc-spark-sql-nlp-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def spark_sql_nlp_pipeline():
    # Step 1: Create a Dataproc cluster
    create_cluster_task = create_dataproc_cluster(
        project_id="your_project_id",
        region="your_region",  # e.g., "us-central1"
        cluster_name="example-cluster",
    )

    # Step 2: Run a Spark SQL batch job
    spark_sql_task = DataprocSparkSqlBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="spark-sql-batch",
        batch={
            "spark_sql_batch": {
                "query_file_uri": "gs://your-bucket/sql/queries.sql",  # SQL script stored in GCS
                "jar_file_uris": [],  # Optional additional JARs
            },
        },
        cluster_name=create_cluster_task.output,  # Use the created cluster
    )

    # Step 3: Process data with a PySpark job (NLP tasks)
    nlp_task = DataprocPySparkBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="nlp-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/nlp_job.py",  # PySpark script
                "args": [
                    "--input_path", "gs://your-bucket/sql_output/result.csv",  # Output from Step 2
                    "--output_path", "gs://your-bucket/nlp_output/processed_result.csv",
                ],
            },
        },
        cluster_name=create_cluster_task.output,  # Use the created cluster
    )
    nlp_task.after(spark_sql_task)  # Ensure it runs after the SQL job

    # Step 4: Delete the Dataproc cluster after all jobs are completed
    delete_cluster_task = delete_dataproc_cluster(
        project_id="your_project_id",
        region="your_region",
        cluster_name=create_cluster_task.output,
    )
    delete_cluster_task.after(nlp_task)  # Ensure deletion happens after NLP task completes
