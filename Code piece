from google.cloud import aiplatform

# Replace with your Project ID & Location
project_id = "your_project_id"
location = "your_region"

# List tuning jobs
tuning_jobs = aiplatform.HyperparameterTuningJob.list(
    project=project_id, location=location
)

# Print results
for job in tuning_jobs:
    print(f"Job Name: {job.display_name}")
    print(f"State: {job.state}")
    print(f"Best trial parameters: {job.best_trial.parameters}")
    print(f"Best ROC AUC: {job.best_trial.metrics['roc_auc']}")



"command": [
    "python3",
    "-c",
    """
import os
from google.cloud import storage

# Fetch hyperparameters passed from Vertex AI
n_estimators = os.environ.get("N_ESTIMATORS")
max_depth = os.environ.get("MAX_DEPTH")
min_samples_split = os.environ.get("MIN_SAMPLES_SPLIT")

# Debugging: Print the values to verify they are correctly passed
print(f"Using hyperparameters: n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}")

# Download the training script from GCS
client = storage.Client()
bucket = client.bucket("hsbc-244716-gecsai-prod-analytics-risk-detection")
blob = bucket.blob("Khamaleswar/Models/train_script_xgb.py")
blob.download_to_filename("/script.py")

# Run the script with hyperparameters
os.system(f"python3 /script.py --n_estimators {n_estimators} --max_depth {max_depth} --min_samples_split {min_samples_split}")
    """
],
"env": [
    {"name": "N_ESTIMATORS", "value": "{{$.trial.parameters.n_estimators}}"},
    {"name": "MAX_DEPTH", "value": "{{$.trial.parameters.max_depth}}"},
    {"name": "MIN_SAMPLES_SPLIT", "value": "{{$.trial.parameters.min_samples_split}}"}
]




worker_pool_specs=[
    {
        "machine_spec": {
            "machine_type": "n1-highmem-96"
        },
        "replica_count": 1,
        "container_spec": {
            "image_uri": "europe-west2-docker.pkg.dev/hsbc-244716-gecsai-prod/vertex-base-images-prod/training_base_ml:2.1.4",
            "command": [
                "python3",
                "-c",
                """
                import os
                from google.cloud import storage
                
                os.environ['N_ESTIMATORS'] = str(${trial.parameters.n_estimators})
                os.environ['MAX_DEPTH'] = str(${trial.parameters.max_depth})
                os.environ['MIN_SAMPLES_SPLIT'] = str(${trial.parameters.min_samples_split})
                
                client = storage.Client()
                bucket = client.bucket('hsbc-244716-gecsai-prod-analytics-risk-detection')
                blob = bucket.blob('Khamaleswar/Models/train_script_xgb.py')
                blob.download_to_filename('/script.py')
                
                os.system('python3 /script.py --n_estimators=${N_ESTIMATORS} --max_depth=${MAX_DEPTH} --min_samples_split=${MIN_SAMPLES_SPLIT}')
                """
            ]
        }
    }
]


os.system('python3 /script.py --n_estimators=${N_ESTIMATORS} --max_depth=${MAX_DEPTH} --min_samples_split=${MIN_SAMPLES_SPLIT}')





"command": [
    "python3", "-c",
    "from google.cloud import storage;"
    "import os;"
    "client = storage.Client();"
    "bucket = client.bucket('hsbc-244716-gecsai-prod-analytics-risk-detection');"
    "blob = bucket.blob('Khamaleswar/Models/train_script_xgb.py');"
    "blob.download_to_filename('/script.py');"
    "os.system(f'python3 /script.py --n_estimators {trial.parameters.n_estimators} --max_depth {trial.parameters.max_depth} --min_samples_split {trial.parameters.min_samples_split}')"
]

from kfp.v2 import compiler
from kfp.v2.dsl import pipeline, component, Output, Dataset
from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp, DataprocClusterCreateOp, DataprocClusterDeleteOp
from google_cloud_pipeline_components.v1.bigquery import BigQueryQueryJobOp

PROJECT_ID = 'your-project-id'
REGION = 'your-region'
BUCKET_NAME = 'your-gcs-bucket'
CLUSTER_NAME = 'vertex-dataproc-cluster'
PYSPARK_FILE_URI = 'gs://your-bucket/path-to-your-pyspark-job.py'

@pipeline(name="vertex-ai-dataproc-pipeline", description="Pipeline with BigQuery and Dataproc integration")
def dataproc_pipeline():
    # Step 1: Fetch data from BigQuery
    bigquery_query = BigQueryQueryJobOp(
        query="""
            SELECT * FROM `your-project-id.your-dataset.your-table`
            LIMIT 1000
        """,
        project=PROJECT_ID,
        location=REGION,
        output_table=f"{PROJECT_ID}.temp_dataset.temp_table",  # Temporary BQ table for staging
        write_disposition="WRITE_TRUNCATE"
    )
    
    # Step 2: Save BigQuery output to GCS
    bigquery_to_gcs = BigQueryQueryJobOp(
        query=f"""
            EXPORT DATA OPTIONS(
                uri='gs://{BUCKET_NAME}/bigquery_output/*.csv',
                format='CSV',
                overwrite=true
            ) AS
            SELECT * FROM `{PROJECT_ID}.temp_dataset.temp_table`
        """,
        project=PROJECT_ID,
        location=REGION
    ).after(bigquery_query)
    
    # Step 3: Create a Dataproc cluster
    create_cluster = DataprocClusterCreateOp(
        project=PROJECT_ID,
        region=REGION,
        cluster_name=CLUSTER_NAME,
        gcs_bucket=BUCKET_NAME,
        cluster_config={
            "master_config": {"num_instances": 1, "machine_type_uri": "n1-standard-4"},
            "worker_config": {"num_instances": 2, "machine_type_uri": "n1-standard-4"},
        }
    ).after(bigquery_to_gcs)
    
    # Step 4: Submit PySpark job to Dataproc
    pyspark_task = DataprocPySparkBatchOp(
        project=PROJECT_ID,
        location=REGION,
        batch_id="pyspark-job",
        main_python_file_uri=PYSPARK_FILE_URI,
        cluster_name=create_cluster.output["cluster_name"],
        args=[
            f"gs://{BUCKET_NAME}/bigquery_output/*.csv",  # Input data path
            f"gs://{BUCKET_NAME}/processed_output/"       # Output data path
        ]
    ).after(create_cluster)
    
    # Step 5: Delete the cluster after the job is complete
    delete_cluster = DataprocClusterDeleteOp(
        project=PROJECT_ID,
        region=REGION,
        cluster_name=create_cluster.output["cluster_name"]
    ).after(pyspark_task)

compiler.Compiler().compile(
    pipeline_func=dataproc_pipeline,
    package_path="vertex_ai_dataproc_with_bigquery_pipeline.json"
)


from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='your-region')

# Trigger the pipeline
job = aiplatform.PipelineJob(
    display_name="vertex-ai-dataproc-pipeline",
    template_path="vertex_ai_dataproc_with_bigquery_pipeline.json",
    pipeline_root="gs://your-gcs-bucket/pipeline-root/",
    parameter_values={}  # Add any parameters if needed
)

job.run()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize stopwords, lemmatizer, and stemmer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("TextProcessing") \
    .getOrCreate()

# Input and output paths
input_path = "gs://your-bucket/bigquery_output/*.csv"  # Input data fetched from BigQuery
output_path = "gs://your-bucket/processed_output/"    # Processed output path

# Load data
df = spark.read.csv(input_path, header=True, inferSchema=True)

# UDFs for text processing
def clean_html(text):
    """Remove HTML tags from text."""
    return BeautifulSoup(text, "html.parser").get_text()

def clean_urls(text):
    """Remove URLs from text."""
    return re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

def remove_stopwords(text):
    """Remove stopwords from text."""
    words = text.split()
    return " ".join([word for word in words if word.lower() not in stop_words])

def lemmatize(text):
    """Perform lemmatization on text."""
    words = text.split()
    return " ".join([lemmatizer.lemmatize(word) for word in words])

def stem(text):
    """Perform stemming on text."""
    words = text.split()
    return " ".join([stemmer.stem(word) for word in words])

def process_text(text):
    """Perform full text processing: HTML removal, URL removal, stopwords removal, lemmatization, and stemming."""
    if not text:
        return ""
    text = clean_html(text)
    text = clean_urls(text)
    text = remove_stopwords(text)
    text = lemmatize(text)
    text = stem(text)
    return text

# Register UDF
process_text_udf = udf(process_text, StringType())

# Apply text processing
processed_df = df.withColumn("processed_text", process_text_udf(col("text")))

# Save processed data
processed_df.write.csv(output_path, header=True, mode="overwrite")

# Stop Spark Session
spark.stop()



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import tensorflow as tf
import nltk

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize stopwords, lemmatizer, and stemmer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("TextProcessingTFRecords") \
    .getOrCreate()

# Input and output paths
input_path = "gs://your-bucket/tfrecords_data/*.tfrecord"  # TFRecords input path
output_path = "gs://your-bucket/processed_output/"        # Processed output path

# Function to parse TFRecords
def parse_example(serialized_example):
    """Parse a single TFRecord example."""
    feature_description = {
        "text": tf.io.FixedLenFeature([], tf.string),  # Text column in TFRecord
    }
    example = tf.io.parse_single_example(serialized_example, feature_description)
    return {"text": example["text"].numpy().decode("utf-8")}

# Load TFRecords as a Spark DataFrame
raw_rdd = spark.sparkContext.newAPIHadoopFile(
    input_path,
    "org.tensorflow.hadoop.io.TFRecordFileInputFormat",
    "org.apache.hadoop.io.BytesWritable",
    "org.apache.hadoop.io.NullWritable",
).map(lambda x: parse_example(x[0].copyBytes()))

df = spark.createDataFrame(raw_rdd)

# UDFs for text processing
def clean_html(text):
    """Remove HTML tags from text."""
    return BeautifulSoup(text, "html.parser").get_text()

def clean_urls(text):
    """Remove URLs from text."""
    return re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

def remove_stopwords(text):
    """Remove stopwords from text."""
    words = text.split()
    return " ".join([word for word in words if word.lower() not in stop_words])

def lemmatize(text):
    """Perform lemmatization on text."""
    words = text.split()
    return " ".join([lemmatizer.lemmatize(word) for word in words])

def stem(text):
    """Perform stemming on text."""
    words = text.split()
    return " ".join([stemmer.stem(word) for word in words])

def process_text(text):
    """Perform full text processing: HTML removal, URL removal, stopwords removal, lemmatization, and stemming."""
    if not text:
        return ""
    text = clean_html(text)
    text = clean_urls(text)
    text = remove_stopwords(text)
    text = lemmatize(text)
    text = stem(text)
    return text

# Register UDF
process_text_udf = udf(process_text, StringType())

# Apply text processing
processed_df = df.withColumn("processed_text", process_text_udf(col("text")))

# Save processed data
processed_df.write.csv(output_path, header=True, mode="overwrite")

# Stop Spark Session
spark.stop()


from google_cloud_pipeline_components.v1.dataproc.create_pyspark_batch.component import dataproc_create_pyspark_batch as DataprocPySparkBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_batch.component import dataproc_create_spark_batch as DataprocSparkBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_r_batch.component import dataproc_create_spark_r_batch as DataprocSparkRBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_sql_batch.component import dataproc_create_spark_sql_batch as DataprocSparkSqlBatchOp


from google_cloud_pipeline_components.v1.bigquery.create_model.component import bigquery_create_model_job as BigqueryCreateModelJobOp
from google_cloud_pipeline_components.v1.bigquery.detect_anomalies_model.component import bigquery_detect_anomalies_job as BigqueryDetectAnomaliesModelJobOp
from google_cloud_pipeline_components.v1.bigquery.drop_model.component import bigquery_drop_model_job as BigqueryDropModelJobOp
from google_cloud_pipeline_components.v1.bigquery.evaluate_model.component import bigquery_evaluate_model_job as BigqueryEvaluateModelJobOp
from google_cloud_pipeline_components.v1.bigquery.explain_forecast_model.component import bigquery_explain_forecast_model_job as BigqueryExplainForecastModelJobOp
from google_cloud_pipeline_components.v1.bigquery.explain_predict_model.component import bigquery_explain_predict_model_job as BigqueryExplainPredictModelJobOp
from google_cloud_pipeline_components.v1.bigquery.export_model.component import bigquery_export_model_job as BigqueryExportModelJobOp
from google_cloud_pipeline_components.v1.bigquery.feature_importance.component import bigquery_ml_feature_importance_job as BigqueryMLFeatureImportanceJobOp
from google_cloud_pipeline_components.v1.bigquery.forecast_model.component import bigquery_forecast_model_job as BigqueryForecastModelJobOp
from google_cloud_pipeline_components.v1.bigquery.global_explain.component import bigquery_ml_global_explain_job as BigqueryMLGlobalExplainJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_advanced_weights.component import bigquery_ml_advanced_weights_job as BigqueryMLAdvancedWeightsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_arima_coefficients.component import bigquery_ml_arima_coefficients as BigqueryMLArimaCoefficientsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_arima_evaluate.component import bigquery_ml_arima_evaluate_job as BigqueryMLArimaEvaluateJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_centroids.component import bigquery_ml_centroids_job as BigqueryMLCentroidsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_confusion_matrix.component import bigquery_ml_confusion_matrix_job as BigqueryMLConfusionMatrixJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_feature_info.component import bigquery_ml_feature_info_job as BigqueryMLFeatureInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_principal_component_info.component import bigquery_ml_principal_component_info_job as BigqueryMLPrincipalComponentInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_principal_components.component import bigquery_ml_principal_components_job as BigqueryMLPrincipalComponentsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_recommend.component import bigquery_ml_recommend_job as BigqueryMLRecommendJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_reconstruction_loss.component import bigquery_ml_reconstruction_loss_job as BigqueryMLReconstructionLossJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_roc_curve.component import bigquery_ml_roc_curve_job as BigqueryMLRocCurveJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_training_info.component import bigquery_ml_training_info_job as BigqueryMLTrainingInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_trial_info.component import bigquery_ml_trial_info_job as BigqueryMLTrialInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_weights.component import bigquery_ml_weights_job as BigqueryMLWeightsJobOp
from google_cloud_pipeline_components.v1.bigquery.predict_model.component import bigquery_predict_model_job as BigqueryPredictModelJobOp
from google_cloud_pipeline_components.v1.bigquery.query_job.component import bigquery_query_job as BigqueryQueryJobOp



from kfp.v2.dsl import pipeline
from google_cloud_pipeline_components.v1.dataproc import DataprocSparkSqlBatchOp, DataprocPySparkBatchOp

@pipeline(name="dataproc-spark-sql-nlp-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def spark_sql_nlp_pipeline():
    # Step 1: Create a Dataproc cluster
    create_cluster_task = create_dataproc_cluster(
        project_id="your_project_id",
        region="your_region",  # e.g., "us-central1"
        cluster_name="example-cluster",
    )

    # Step 2: Run a Spark SQL batch job
    spark_sql_task = DataprocSparkSqlBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="spark-sql-batch",
        batch={
            "spark_sql_batch": {
                "query_file_uri": "gs://your-bucket/sql/queries.sql",  # SQL script stored in GCS
                "jar_file_uris": [],  # Optional additional JARs
            },
        },
        cluster_name=create_cluster_task.output,  # Use the created cluster
    )

    # Step 3: Process data with a PySpark job (NLP tasks)
    nlp_task = DataprocPySparkBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="nlp-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/nlp_job.py",  # PySpark script
                "args": [
                    "--input_path", "gs://your-bucket/sql_output/result.csv",  # Output from Step 2
                    "--output_path", "gs://your-bucket/nlp_output/processed_result.csv",
                ],
            },
        },
        cluster_name=create_cluster_task.output,  # Use the created cluster
    )
    nlp_task.after(spark_sql_task)  # Ensure it runs after the SQL job

    # Step 4: Delete the Dataproc cluster after all jobs are completed
    delete_cluster_task = delete_dataproc_cluster(
        project_id="your_project_id",
        region="your_region",
        cluster_name=create_cluster_task.output,
    )
    delete_cluster_task.after(nlp_task)  # Ensure deletion happens after NLP task completes



from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
import re
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder.appName("NLP Tasks").getOrCreate()

# Define helper functions
def remove_html(text):
    return BeautifulSoup(text, "html.parser").get_text()

def remove_urls(text):
    return re.sub(r"http\S+|www.\S+", "", text)

def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    return " ".join([word for word in text.split() if word.lower() not in stop_words])

def stem_text(text):
    stemmer = PorterStemmer()
    return " ".join([stemmer.stem(word) for word in text.split()])

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

# Register UDFs
remove_html_udf = udf(remove_html, StringType())
remove_urls_udf = udf(remove_urls, StringType())
remove_stopwords_udf = udf(remove_stopwords, StringType())
stem_text_udf = udf(stem_text, StringType())
lemmatize_text_udf = udf(lemmatize_text, StringType())

# Input and output paths
input_path = "gs://your-bucket/sql_output/result.csv"  # Replace with the input path
output_path = "gs://your-bucket/nlp_output/processed_result.csv"  # Replace with the output path

# Read input data
df = spark.read.csv(input_path, header=True)

# Apply NLP transformations
df = df.withColumn("text_no_html", remove_html_udf(col("text_column")))
df = df.withColumn("text_no_urls", remove_urls_udf(col("text_no_html")))
df = df.withColumn("text_no_stopwords", remove_stopwords_udf(col("text_no_urls")))
df = df.withColumn("stemmed_text", stem_text_udf(col("text_no_stopwords")))
df = df.withColumn("lemmatized_text", lemmatize_text_udf(col("stemmed_text")))

# Write the result back to GCS
df.write.csv(output_path, header=True)


from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("BigQuery-Spark Integration") \
    .getOrCreate()

# Define configurations
project_id = "your_project_id"
input_table = "your_project_id.your_dataset.your_table"
output_table = "your_project_id.your_dataset.output_table"

# Read data from BigQuery into a Spark DataFrame
df = spark.read.format("bigquery") \
    .option("project", project_id) \
    .option("table", input_table) \
    .load()

# Perform a transformation (e.g., filtering rows)
transformed_df = df.filter(df["column_name"] == "some_value")

# Write the transformed data back to BigQuery
transformed_df.write.format("bigquery") \
    .option("table", output_table) \
    .option("writeMethod", "direct") \  # Options: 'direct' (default) or 'indirect'
    .save()

# Stop the Spark session
spark.stop()


from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp
from kfp.v2.dsl import pipeline

@pipeline(name="bigquery-spark-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def bigquery_spark_pipeline():
    spark_job = DataprocPySparkBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="bigquery-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/bigquery_spark_job.py",  # Path to PySpark script
            },
        },
        cluster_name="your-dataproc-cluster",  # Use a pre-existing or dynamically created cluster
    )



from kfp.v2.dsl import pipeline, component
from kfp.v2.compiler import Compiler
from google.cloud import dataproc_v1
from google.cloud import aiplatform
import time


@component
def create_dataproc_cluster(project: str, region: str, cluster_name: str):
    """Creates a Dataproc cluster."""
    cluster_client = dataproc_v1.ClusterControllerClient(client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"})
    cluster_config = {
        "project_id": project,
        "config": {
            "gce_cluster_config": {
                "zone_uri": f"{region}-b",  # Change to preferred zone
            },
            "master_config": {
                "num_instances": 1,
                "machine_type_uri": "n1-standard-4",
            },
            "worker_config": {
                "num_instances": 2,
                "machine_type_uri": "n1-standard-4",
            },
        },
    }
    operation = cluster_client.create_cluster(
        request={"project_id": project, "region": region, "cluster": {"cluster_name": cluster_name, **cluster_config}}
    )
    result = operation.result()
    print(f"Cluster created: {result.cluster_name}")


@component
def delete_dataproc_cluster(project: str, region: str, cluster_name: str):
    """Deletes a Dataproc cluster."""
    cluster_client = dataproc_v1.ClusterControllerClient(client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"})
    operation = cluster_client.delete_cluster(
        request={"project_id": project, "region": region, "cluster_name": cluster_name}
    )
    operation.result()
    print(f"Cluster deleted: {cluster_name}")


@pipeline(name="bigquery-spark-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def bigquery_spark_pipeline():
    project = "your_project_id"
    region = "europe-west2"
    cluster_name = f"bigquery-spark-cluster-{int(time.time())}"  # Unique cluster name

    # Step 1: Create a Dataproc cluster
    create_cluster_task = create_dataproc_cluster(project=project, region=region, cluster_name=cluster_name)

    # Step 2: Submit the PySpark job
    pyspark_job_task = DataprocPySparkBatchOp(
        project=project,
        location=region,
        batch_id="bigquery-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/bigquery_spark_job.py",
            },
        },
        cluster_name=cluster_name,
    )
    pyspark_job_task.after(create_cluster_task)

    # Step 3: Delete the Dataproc cluster after the job is complete
    delete_cluster_task = delete_dataproc_cluster(project=project, region=region, cluster_name=cluster_name)
    delete_cluster_task.after(pyspark_job_task)


# Compile the pipeline
Compiler().compile(
    pipeline_func=bigquery_spark_pipeline,
    package_path="bigquery_spark_pipeline.json"
)

# Submit the pipeline to Vertex AI
aiplatform.init(project="your_project_id", location="europe-west2")

pipeline_job = aiplatform.PipelineJob(
    display_name="bigquery-spark-pipeline",
    template_path="bigquery_spark_pipeline.json",
)

pipeline_job.run(sync=True)


from google.cloud import dataproc_v1

# Initialize the client with the correct endpoint
region = "europe-west2"
client = dataproc_v1.ClusterControllerClient(
    client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"}
)

# Project and cluster details
project_id = "your_project_id"
cluster_name = "example-cluster"
zone = f"{region}-b"  # Adjust the zone as needed

# Cluster configuration
cluster_config = {
    "project_id": project_id,
    "cluster_name": cluster_name,
    "config": {
        "gce_cluster_config": {
            "zone_uri": zone,
        },
        "master_config": {
            "num_instances": 1,
            "machine_type_uri": "n1-standard-4",
        },
        "worker_config": {
            "num_instances": 2,
            "machine_type_uri": "n1-standard-4",
        },
    },
}

# Create the cluster
operation = client.create_cluster(
    request={"project_id": project_id, "region": region, "cluster": cluster_config}
)
cluster = operation.result()
print(f"Cluster created: {cluster.cluster_name}")

# Delete the cluster
delete_operation = client.delete_cluster(
    request={"project_id": project_id, "region": region, "cluster_name": cluster_name}
)
delete_operation.result()
print(f"Cluster deleted: {cluster_name}")


try:
    client = dataproc_v1.ClusterControllerClient(
        client_options={"api_endpoint": "europe-west2-dataproc.googleapis.com:443"}
    )
    print("Client initialized successfully!")
except Exception as e:
    print(f"Failed to initialize client: {e}")


import subprocess
from kfp.v2.dsl import component

@component
def gcloud_create_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Creates a Dataproc cluster using gcloud CLI."""
    command = [
        "gcloud", "dataproc", "clusters", "create", cluster_name,
        "--project", project_id,
        "--region", region,
        "--zone", f"{region}-b",  # Adjust zone if needed
        "--master-machine-type", "n1-standard-4",
        "--worker-machine-type", "n1-standard-4",
        "--num-workers", "2",
    ]
    subprocess.run(command, check=True)
    print(f"Cluster {cluster_name} created.")

@component
def gcloud_delete_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Deletes a Dataproc cluster using gcloud CLI."""
    command = [
        "gcloud", "dataproc", "clusters", "delete", cluster_name,
        "--project", project_id,
        "--region", region,
        "--quiet",  # Skips confirmation prompt
    ]
    subprocess.run(command, check=True)
    print(f"Cluster {cluster_name} deleted.")



import google.auth
import requests
from kfp.v2.dsl import component

@component
def rest_create_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Creates a Dataproc cluster using REST API."""
    credentials, _ = google.auth.default()
    access_token = credentials.token
    endpoint = f"https://dataproc.googleapis.com/v1/projects/{project_id}/regions/{region}/clusters"

    cluster_config = {
        "clusterName": cluster_name,
        "config": {
            "gceClusterConfig": {"zoneUri": f"{region}-b"},
            "masterConfig": {"numInstances": 1, "machineTypeUri": "n1-standard-4"},
            "workerConfig": {"numInstances": 2, "machineTypeUri": "n1-standard-4"},
        },
    }

    headers = {"Authorization": f"Bearer {access_token}"}
    response = requests.post(endpoint, json=cluster_config, headers=headers)

    if response.status_code == 200:
        print(f"Cluster {cluster_name} created.")
    else:
        print(f"Failed to create cluster: {response.text}")

@component
def rest_delete_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Deletes a Dataproc cluster using REST API."""
    credentials, _ = google.auth.default()
    access_token = credentials.token
    endpoint = f"https://dataproc.googleapis.com/v1/projects/{project_id}/regions/{region}/clusters/{cluster_name}"

    headers = {"Authorization": f"Bearer {access_token}"}
    response = requests.delete(endpoint, headers=headers)

    if response.status_code == 200:
        print(f"Cluster {cluster_name} deleted.")
    else:
        print(f"Failed to delete cluster: {response.text}")



from kfp.v2.dsl import pipeline

@pipeline(name="dataproc-cluster-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def dataproc_pipeline():
    project_id = "your_project_id"
    region = "europe-west2"
    cluster_name = "example-cluster"

    # Step 1: Create Dataproc cluster
    create_task = gcloud_create_dataproc_cluster(
        project_id=project_id,
        region=region,
        cluster_name=cluster_name,
    )

    # Step 2: Delete Dataproc cluster
    delete_task = gcloud_delete_dataproc_cluster(
        project_id=project_id,
        region=region,
        cluster_name=cluster_name,
    )
    delete_task.after(create_task)
 
# Copyright 2023 The Kubeflow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, List

from google_cloud_pipeline_components import _image
from google_cloud_pipeline_components import _placeholders
from kfp.dsl import ConcatPlaceholder
from kfp.dsl import container_component
from kfp.dsl import ContainerSpec
from kfp.dsl import OutputPath


@container_component
def dataproc_create_pyspark_batch(
    main_python_file_uri: str,
    gcp_resources: OutputPath(str),
    location: str = 'us-central1',
    batch_id: str = '',
    labels: Dict[str, str] = {},
    container_image: str = '',
    runtime_config_version: str = '',
    runtime_config_properties: Dict[str, str] = {},
    service_account: str = '',
    network_tags: List[str] = [],
    kms_key: str = '',
    network_uri: str = '',
    subnetwork_uri: str = '',
    metastore_service: str = '',
    spark_history_dataproc_cluster: str = '',
    python_file_uris: List[str] = [],
    jar_file_uris: List[str] = [],
    file_uris: List[str] = [],
    archive_uris: List[str] = [],
    args: List[str] = [],
    project: str = _placeholders.PROJECT_ID_PLACEHOLDER,
):
  # fmt: off
  """Create a Dataproc PySpark batch workload and wait for it to finish.

  Args:
      location: Location of the Dataproc batch workload. If not set, defaults to `"us-central1"`.
      batch_id: The ID to use for the batch, which will become the final component of the batch's resource name. If none is specified, a default name will be generated by the component.  This value must be 4-63 characters. Valid characters are `/[a-z][0-9]-/`.
      labels: The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035. Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035. No more than 32 labels can be associated with a batch.  An object containing a list of `"key": value` pairs. Example: `{ "name": "wrench", "mass": "1.3kg", "count": "3" }`.
      container_image: Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
      runtime_config_version: Version of the batch runtime.
      runtime_config_properties: Runtime configuration for the workload.
      service_account: Service account that is used to execute the workload.
      network_tags: Tags used for network traffic control.
      kms_key: The Cloud KMS key to use for encryption.
      network_uri: Network URI to connect workload to.
      subnetwork_uri: Subnetwork URI to connect workload to.
      metastore_service: Resource name of an existing Dataproc Metastore service.
      spark_history_dataproc_cluster: The Spark History Server configuration for the workload.
      main_python_file_uri: The HCFS URI of the main Python file to use as the Spark driver. Must be a `.py` file.
      python_file_uris: HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: `.py`, `.egg`, and `.zip`.
      jar_file_uris: HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
      file_uris: HCFS URIs of files to be placed in the working directory of each executor.
      archive_uris: HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
      args: The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.
      project: Project to run the Dataproc batch workload. Defaults to the project in which the PipelineJob is run.

  Returns:
      gcp_resources: Serialized gcp_resources proto tracking the Dataproc batch workload. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
  """
  # fmt: on
  return ContainerSpec(
      image=_image.GCPC_IMAGE_TAG,
      command=[
          'python3',
          '-u',
          '-m',
          'google_cloud_pipeline_components.container.v1.dataproc.create_pyspark_batch.launcher',
      ],
      args=[
          '--type',
          'DataprocPySparkBatch',
          '--payload',
          ConcatPlaceholder([
              '{',
              '"labels": ',
              labels,
              ', "runtime_config": {',
              '"version": "',
              runtime_config_version,
              '"',
              ', "container_image": "',
              container_image,
              '"',
              ', "properties": ',
              runtime_config_properties,
              '}',
              ', "environment_config": {',
              '"execution_config": {',
              '"service_account": "',
              service_account,
              '"',
              ', "network_tags": ',
              network_tags,
              ', "kms_key": "',
              kms_key,
              '"',
              ', "network_uri": "',
              network_uri,
              '"',
              ', "subnetwork_uri": "',
              subnetwork_uri,
              '"',
              '}',
              ', "peripherals_config": {',
              '"metastore_service": "',
              metastore_service,
              '"',
              ', "spark_history_server_config": { ',
              '"dataproc_cluster": "',
              spark_history_dataproc_cluster,
              '"',
              '}',
              '}',
              '}',
              ', "pyspark_batch": {',
              '"main_python_file_uri": "',
              main_python_file_uri,
              '"',
              ', "python_file_uris": ',
              python_file_uris,
              ', "jar_file_uris": ',
              jar_file_uris,
              ', "file_uris": ',
              file_uris,
              ', "archive_uris": ',
              archive_uris,
              ', "args": ',
              args,
              '}',
              '}',
          ]),
          '--project',
          project,
          '--location',
          location,
          '--batch_id',
          batch_id,
          '--gcp_resources',
          gcp_resources,
      ],
  )

from pyspark.sql import SparkSession

def save_dataframe_to_gcs():
    # Initialize SparkSession
    spark = SparkSession.builder \
        .appName("SaveDataFrameToParquet") \
        .getOrCreate()

    # Example data to create a DataFrame
    data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
    columns = ["Name", "Age"]
    df = spark.createDataFrame(data, columns)

    # GCS bucket path
    gcs_bucket = "gs://your-gcs-bucket-name/path/to/output"

    # Save DataFrame as Parquet to GCS
    df.write \
        .mode("overwrite") \  # or "append"
        .parquet(gcs_bucket)

    print(f"DataFrame successfully saved to {gcs_bucket}")

if __name__ == "__main__":
    save_dataframe_to_gcs()


from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp

dataproc_pyspark_op = DataprocPySparkBatchOp(
    project="your-gcp-project-id",
    location="your-region",
    batch_id="your-batch-id",
    pyspark_file="gs://your-gcs-bucket-name/path/to/save_to_parquet.py",
    args=[],  # Pass any additional args if needed
    subnetwork_uri="projects/your-project/regions/your-region/subnetworks/your-subnetwork",  # Optional, if needed
    temp_bucket="your-temp-gcs-bucket",
    container_image="gcr.io/your-project-id/pyspark-image:latest",  # Your custom container with PySpark
)

from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("BQ_to_GCS").getOrCreate()

# Define BigQuery and GCS details
bq_table = "your_project.dataset.your_table"
gcs_output_path = "gs://your_bucket/bq_data"

# Load data from BigQuery
df = spark.read.format("bigquery").option("table", bq_table).load()

# Save to GCS in Parquet format
df.write.mode("overwrite").parquet(gcs_output_path)

print(f"Data saved to {gcs_output_path}")
spark.stop()


from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover

# Initialize Spark
spark = SparkSession.builder.appName("NLP_Processing").getOrCreate()

# GCS Input & Output Paths
input_path = "gs://your_bucket/bq_data"
output_path = "gs://your_bucket/nlp_data"

# Load Data
df = spark.read.parquet(input_path)

# NLP Processing: Tokenization & Stopword Removal
tokenizer = Tokenizer(inputCol="text_column", outputCol="words")
df = tokenizer.transform(df)

remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
df = remover.transform(df)

# Save Transformed Data
df.write.mode("overwrite").parquet(output_path)

print(f"NLP Transformed Data saved to {output_path}")
spark.stop()



from google.cloud import dataproc_v1

# Set up the Dataproc batch client
client = dataproc_v1.BatchControllerClient()

# Define your project, region, and batch name
project_id = "your-project-id"
region = "your-region"
batch_id = "pyspark-optimization-batch"

# Define the batch job request
batch = {
    "pyspark_batch": {
        "main_python_file_uri": "gs://your-bucket/path/to/script.py",
        "args": [],  # Add any command-line arguments if needed
        "jar_file_uris": [],  # If you need any additional JARs
        "python_file_uris": [],  # If you have additional Python files
        "properties": {
            "spark.driver.memory": "12g",
            "spark.executor.memory": "12g",
            "spark.executor.cores": "4",
            "spark.default.parallelism": "200",
            "spark.sql.shuffle.partitions": "200",
            "spark.shuffle.service.enabled": "true",
            "spark.sql.autoBroadcastJoinThreshold": "52428800",
            "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
            "spark.kryoserializer.buffer.max": "512m",
            "spark.speculation": "true",
            "spark.executor.extraJavaOptions": "-XX:+UseG1GC",
            "spark.sql.parquet.compression.codec": "snappy",
        }
    },
    "environment_config": {
        "execution_config": {
            "service_account": "your-service-account@your-project.iam.gserviceaccount.com",
            "subnetwork_uri": "projects/your-project/regions/your-region/subnetworks/your-subnet",
        }
    }
}

# Submit the batch job
operation = client.create_batch(
    request={"parent": f"projects/{project_id}/locations/{region}", "batch": batch, "batch_id": batch_id}
)

# Wait for the job to complete
operation.result()
print(f"Batch job {batch_id} submitted successfully!")



query = """
    SELECT * FROM `your_project.your_dataset.your_table`
    WHERE RAND() < 0.1
    LIMIT 100
"""

df = spark.read.format("bigquery") \
    .option("query", query) \
    .load()

df.show()


from pyspark.sql.functions import col

df = df.withColumn("Recipient", col("Recipient").cast("string"))

from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, ArrayType, StructType, StructField

# Define the schema (assuming the data contains a list of strings)
schema = ArrayType(StringType()) 

# Apply the transformation
df = df.withColumn("Recipient", from_json(col("Recipient"), schema))
df = df.withColumn("Message", from_json(col("Message"), schema))

from pyspark.sql.functions import col

df = (
    df.withColumn("Message", col("Message").cast("string"))
      .withColumn("all_names", col("all_names").cast("string"))
      .withColumn("Lexicon_LineNo", col("Lexicon_LineNo").cast("string"))
      .withColumn("Expanded_hit", col("Expanded_hit").cast("string"))
)





from pyspark.sql.functions import col, when, udf
from pyspark.sql.types import StringType, ArrayType, IntegerType

# Define UDFs for transformations
html_remover_lex_udf = udf(html_remover_lex, StringType())
url_remover_udf = udf(url_remover, StringType())
text_cleaning_lex_udf = udf(text_cleaning_lex, StringType())
chat_expander_udf = udf(chat_expander, StringType())
name_finder_udf = udf(lambda sender, recipient, message, expanded_msg: 
                      name_finder(sender, recipient, message, expanded_msg), StringType())
lex_line_udf = udf(lambda expanded_msg, lex_cleaned: 
                   lex_line(expanded_msg, lex_cleaned), ArrayType(StringType()))
lex_based_counter_udf = udf(lambda expanded_msg, lex_line_no: 
                            lex_based_counter(expanded_msg, lex_line_no), IntegerType())

# Apply transformations
df = df.withColumn("Csurv_Lexicon_html_removed", html_remover_lex_udf(col("Csurv_Lexicon")))
df = df.withColumn("Csurv_Lexicon_url_removed", url_remover_udf(col("Csurv_Lexicon_html_removed")))
df = df.withColumn("Csurv_Lexicon_cleaned", text_cleaning_lex_udf(col("Csurv_Lexicon_url_removed")))

df = df.withColumn("Expanded_Message", chat_expander_udf(col("Message")))

df = df.withColumn("all_names", name_finder_udf(col("Sender"), col("Recipient"), col("Message"), col("Expanded_Message")))

df = df.withColumn("Lexicon_LineNo", lex_line_udf(col("Expanded_Message"), col("Csurv_Lexicon_cleaned")))

df = df.withColumn("Expanded_hit", lex_based_counter_udf(col("Expanded_Message"), col("Lexicon_LineNo")))

df = df.withColumn("Match_Indicator", when(col("Lexicon_LineNo").isNotNull() & (col("Lexicon_LineNo") != ""), 1).otherwise(0))


df = df.filter(col("KI_Name").isin(["GMM01", "I001", "IIRS01"]))






from google.cloud import aiplatform

job = aiplatform.CustomJob(
    display_name="gpu-training-job",
    worker_pool_specs=[
        {
            "machine_spec": {
                "machine_type": "n1-standard-4",  # Adjust machine type as needed
                "accelerator_type": "NVIDIA_TESLA_T4",  # Change based on available GPU
                "accelerator_count": 1,  # Number of GPUs
            },
            "replica_count": 1,
            "container_spec": {
                "image_uri": "gcr.io/your-project-id/your-image",  # Your Docker image with dependencies
            },
        }
    ],
)

job.run()



from kfp.v2.dsl import component, pipeline

@component(base_image="python:3.8")
def train_model():
    import tensorflow as tf
    print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

@pipeline(name="gpu-pipeline")
def pipeline():
    train_model().set_cpu_limit("4").set_memory_limit("16G").set_gpu_limit(1)


from kfp.v2.dsl import component, pipeline

@component(base_image="nvidia/cuda:11.2.2-runtime-ubuntu20.04")  # Ensure CUDA support
def train_model():
    import tensorflow as tf
    print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

@pipeline(name="gpu-pipeline")
def pipeline():
    train_model().set_cpu_limit("4").set_memory_limit("16G").set_gpu_limit(1)


from kfp.v2 import compiler
from google.cloud import aiplatform

compiler.Compiler().compile(pipeline_func=pipeline, package_path="gpu_pipeline.json")

job = aiplatform.PipelineJob(
    display_name="gpu-test-pipeline",
    template_path="gpu_pipeline.json",
    enable_caching=False,
    pipeline_root="gs://your-bucket/pipeline-root",
)

job.run(service_account="your-service-account@your-project.iam.gserviceaccount.com")



@component(base_image="nvidia/cuda:11.2.2-runtime-ubuntu20.04")
def debug_gpu():
    import tensorflow as tf
    import torch
    
    print("TensorFlow GPUs:", tf.config.list_physical_devices('GPU'))
    print("PyTorch CUDA available:", torch.cuda.is_available())
    print("PyTorch GPU count:", torch.cuda.device_count())

@pipeline(name="gpu-debug-pipeline")
def pipeline():
    debug_gpu().set_gpu_limit(1)




from kfp.v2.dsl import component, pipeline

@component(base_image="nvidia/cuda:11.2.2-runtime-ubuntu20.04")  # Ensure CUDA support
def train_model():
    import tensorflow as tf
    print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

    # Enable memory growth for all GPUs
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    # Use all GPUs with MirroredStrategy
    strategy = tf.distribute.MirroredStrategy()
    with strategy.scope():
        # Example: Perform matrix multiplication on multiple GPUs
        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
        b = tf.constant([[5.0, 6.0], [7.0, 8.0]])
        c = tf.matmul(a, b)
        print("Matrix multiplication result on multiple GPUs:", c.numpy())

@pipeline(name="multi-gpu-pipeline")
def pipeline():
    train_model().set_cpu_limit("4").set_memory_limit("16G").set_gpu_limit(2)  # Adjust GPU limit based on availability


from kfp.v2 import compiler
from google.cloud import aiplatform

compiler.Compiler().compile(pipeline_func=pipeline, package_path="multi_gpu_pipeline.json")

job = aiplatform.PipelineJob(
    display_name="multi-gpu-test-pipeline",
    template_path="multi_gpu_pipeline.json",
    enable_caching=False,
    pipeline_root="gs://your-bucket/pipeline-root",
)

job.run(service_account="your-service-account@your-project.iam.gserviceaccount.com")



gpu_devices = tf.config.list_physical_devices('GPU')
for i, gpu in enumerate(gpu_devices):
    with tf.device(f'/GPU:{i}'):
        # Example computation on each GPU
        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
        b = tf.constant([[5.0, 6.0], [7.0, 8.0]])
        c = tf.matmul(a, b)
        print(f"GPU {i} computation result:", c.numpy())



import argparse
import pandas as pd
import joblib
import json
from google.cloud import storage
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

def load_data_from_gcs(gcs_path):
    """Loads a CSV file from Google Cloud Storage into a Pandas DataFrame."""
    storage_client = storage.Client()
    bucket_name, file_path = gcs_path.replace("gs://", "").split("/", 1)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_path)

    df = pd.read_csv(blob.open("r"))
    return df

def train_and_evaluate(train_data_path, target_column, n_estimators, max_depth, min_samples_split, min_samples_leaf):
    """Trains and evaluates a Random Forest model with given hyperparameters."""
    
    df = load_data_from_gcs(train_data_path)
    
    # Split into features and target
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train Model
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42
    )

    model.fit(X_train, y_train)
    y_pred_probs = model.predict_proba(X_test)[:, 1]
    auc_score = roc_auc_score(y_test, y_pred_probs)

    print(f"ROC AUC Score: {auc_score}")

    # Save model if it's the best
    joblib.dump(model, "model.pkl")

    # Save best AUC
    with open("metrics.json", "w") as f:
        json.dump({"auc": auc_score}, f)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-data", type=str, required=True)
    parser.add_argument("--target-column", type=str, required=True)
    parser.add_argument("--n-estimators", type=int, required=True)
    parser.add_argument("--max-depth", type=int, required=True)
    parser.add_argument("--min-samples-split", type=int, required=True)
    parser.add_argument("--min-samples-leaf", type=int, required=True)

    args = parser.parse_args()

    train_and_evaluate(
        args.train_data,
        args.target_column,
        args.n_estimators,
        args.max_depth,
        args.min_samples_split,
        args.min_samples_leaf
    )


import kfp
from kfp.v2 import dsl
from google_cloud_pipeline_components import aiplatform as gcc_aip

# ---------------- CONFIG ----------------
PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"
BUCKET_NAME = "your-gcs-bucket"
TRAINING_DATA = f"gs://{BUCKET_NAME}/data/dataset.csv"
IMAGE_URI = "us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest"

@dsl.pipeline(
    name="rf-hyperparam-tuning-pipeline",
    pipeline_root=f"gs://{BUCKET_NAME}/pipeline-root"
)
def rf_hyperparam_tuning_pipeline():
    """Vertex AI Pipeline for Random Forest Hyperparameter Tuning."""

    hp_tuning_task = gcc_aip.HyperparameterTuningJobRunOp(
        display_name="rf-hyperparam-tuning",
        project=PROJECT_ID,
        location=REGION,
        custom_job=gcc_aip.CustomJob(
            display_name="rf-training-job",
            worker_pool_specs=[
                {
                    "machine_spec": {"machine_type": "n1-standard-4"},
                    "replica_count": 1,
                    "container_spec": {
                        "image_uri": IMAGE_URI,
                        "command": ["python3", "-m", "trainer.task"],
                        "args": [
                            "--train-data", TRAINING_DATA,
                            "--target-column", "target",
                            "--n-estimators", "{n_estimators}",
                            "--max-depth", "{max_depth}",
                            "--min-samples-split", "{min_samples_split}",
                            "--min-samples-leaf", "{min_samples_leaf}"
                        ],
                    },
                }
            ],
        ),
        metric_spec={"auc": "maximize"},
        parameter_spec={
            "n_estimators": gcc_aip.IntegerParameterSpec(min=50, max=500, scale="linear"),
            "max_depth": gcc_aip.IntegerParameterSpec(min=3, max=20, scale="linear"),
            "min_samples_split": gcc_aip.IntegerParameterSpec(min=2, max=10, scale="linear"),
            "min_samples_leaf": gcc_aip.IntegerParameterSpec(min=1, max=5, scale="linear"),
        },
        max_trial_count=10,
        parallel_trial_count=2,
    )

# ---------------- COMPILE PIPELINE ----------------
if __name__ == "__main__":
    from kfp.v2 import compiler
    compiler.Compiler().compile(
        pipeline_func=rf_hyperparam_tuning_pipeline,
        package_path="rf_hyperparam_tuning_pipeline.json"
    )




from google.cloud import aiplatform

PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"

aiplatform.init(project=PROJECT_ID, location=REGION)

job = aiplatform.PipelineJob(
    display_name="rf-hyperparam-tuning-pipeline",
    template_path="rf_hyperparam_tuning_pipeline.json",
    pipeline_root=f"gs://{BUCKET_NAME}/pipeline-root",
)

job.run()




from kfp.v2 import compiler, dsl
from kfp.v2.dsl import component
from google_cloud_pipeline_components import aiplatform as gcc_aip

PROJECT_ID = "your-gcp-project-id"
BUCKET_NAME = "your-gcs-bucket-name"
REGION = "us-central1"

# Path to script in GCS
TRAINING_SCRIPT_PATH = f"gs://{BUCKET_NAME}/scripts/script.py"

# Define pipeline
@dsl.pipeline(
    name="rf-hyperparameter-tuning-pipeline",
    pipeline_root=f"gs://{BUCKET_NAME}/pipeline-root",
)
def rf_hyperparameter_tuning_pipeline():
    
    # Hyperparameter Tuning Job
    hptuning_op = gcc_aip.HyperparameterTuningJobRunOp(
        project=PROJECT_ID,
        location=REGION,
        display_name="rf-hyperparameter-tuning",
        worker_pool_specs=[
            {
                "machine_spec": {
                    "machine_type": "n1-standard-4"
                },
                "replica_count": 1,
                "container_spec": {
                    "image_uri": "us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest",  # Prebuilt Sklearn container
                    "command": [
                        "bash", "-c",  # Run a bash command to download and execute script
                        f"gsutil cp {TRAINING_SCRIPT_PATH} script.py && python3 script.py " +
                        "--training_data gs://your-bucket/training-data.csv " +
                        "--test_data gs://your-bucket/test-data.csv " +
                        "--n_estimators ${trial.parameters.n_estimators} " +
                        "--max_depth ${trial.parameters.max_depth} " +
                        "--min_samples_split ${trial.parameters.min_samples_split} " +
                        "--output_model gs://your-bucket/models/model.joblib"
                    ]
                }
            }
        ],
        study_spec_metrics=[{"metric_id": "accuracy", "goal": "MAXIMIZE"}],
        study_spec_parameters=[
            {"parameter_id": "n_estimators", "discrete_values": [10, 50, 100], "scale_type": "UNIT_LINEAR_SCALE"},
            {"parameter_id": "max_depth", "integer_value_spec": {"min_value": 5, "max_value": 50}, "scale_type": "UNIT_LINEAR_SCALE"},
            {"parameter_id": "min_samples_split", "integer_value_spec": {"min_value": 2, "max_value": 10}, "scale_type": "UNIT_LINEAR_SCALE"}
        ],
        max_trial_count=10,
        parallel_trial_count=2
    )

# Compile the pipeline
compiler.Compiler().compile(
    pipeline_func=rf_hyperparameter_tuning_pipeline,
    package_path="rf_hyperparameter_tuning_pipeline.json",
)



gsutil cp script.py gs://your-gcs-bucket/scripts/script.py

import argparse
import joblib
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import os

# Argument parsing for hyperparameters
parser = argparse.ArgumentParser()
parser.add_argument("--n_estimators", type=int, default=100)
parser.add_argument("--max_depth", type=int, default=None)
parser.add_argument("--min_samples_split", type=int, default=2)
parser.add_argument("--training_data", type=str, required=True)
parser.add_argument("--test_data", type=str, required=True)
parser.add_argument("--output_model", type=str, default="/tmp/model.joblib")

args = parser.parse_args()

# Load dataset
train_df = pd.read_csv(args.training_data)
test_df = pd.read_csv(args.test_data)

X_train, y_train = train_df.iloc[:, :-1], train_df.iloc[:, -1]
X_test, y_test = test_df.iloc[:, :-1], test_df.iloc[:, -1]

# Train Random Forest Model
model = RandomForestClassifier(
    n_estimators=args.n_estimators,
    max_depth=args.max_depth,
    min_samples_split=args.min_samples_split,
    random_state=42
)
model.fit(X_train, y_train)

# Evaluate model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy}")

# Save model
os.makedirs(os.path.dirname(args.output_model), exist_ok=True)
joblib.dump(model, args.output_model)



"command": [
    "python3", "-c",
    "from google.cloud import storage; "
    "import os; "
    "client = storage.Client(); "
    "bucket = client.bucket('your-bucket-name'); "
    "blob = bucket.blob('path/to/script.py'); "
    "blob.download_to_filename('/script.py'); "
    "os.system('python3 /script.py --n_estimators ${trial.parameters.n_estimators} "
    "--max_depth ${trial.parameters.max_depth} --min_samples_split ${trial.parameters.min_samples_split}')"
]




custom_job = aip.CustomJob.from_local_script(
    display_name="xgb-hyperparam-tuning",
    script_path="train_script.py",
    container_uri=config["BASE_IMAGE"],
    args=[
        "--n_estimators", "{{trial.parameters.n_estimators}}",
        "--max_depth", "{{trial.parameters.max_depth}}",
        "--learning_rate", "{{trial.parameters.learning_rate}}"
    ],
    requirements=["scikit-learn", "numpy", "pandas", "joblib", "google-cloud-aiplatform"],
    machine_type="n1-highmem-96",
    staging_bucket="gs://hsbc-244716-gecsai-dev-mlops-tech-pipelines/pipeline_root/models",
    encryption_spec_key_name=PIPELINE_KMS_KEY,
)



import os

# Read hyperparameters from environment variables
n_estimators = int(os.getenv("N_ESTIMATORS", 100))  # Default: 100
max_depth = int(os.getenv("MAX_DEPTH", 10))         # Default: 10
learning_rate = float(os.getenv("LEARNING_RATE", 0.01))  # Default: 0.01

print(f"Using n_estimators={n_estimators}, max_depth={max_depth}, learning_rate={learning_rate}")

# Proceed with training...

custom_job = aiplatform.CustomJob.from_local_script(
    display_name="xgb-hyperparam-tuning",
    script_path="train_script.py",  # Ensure this script is saved as train_script.py
    container_uri=CONFIG["BASE_IMAGE"],
    machine_type="n1-highmem-96",
    staging_bucket="gs://hsbc-244716-gecsai-dev-mlops-tech-pipelines/pipeline_root/models",
    encryption_spec_key_name=PIPELINE_KMS_KEY,
    # Setting hyperparameters as environment variables
    environment_variables={
        "N_ESTIMATORS": "{{$.trial.parameters.n_estimators}}",
        "MAX_DEPTH": "{{$.trial.parameters.max_depth}}",
        "LEARNING_RATE": "{{$.trial.parameters.learning_rate}}"
    },
)


Great question  Vertex AI Workbench and Cloud Workstations in Vertex AI are both development environments provided by Google Cloud, but they serve slightly different purposes and offer different user experiences. Here's a clear breakdown of their differences:


---

 Vertex AI Workbench vs. Vertex AI Cloud Workstations

Feature	Vertex AI Workbench	Vertex AI Cloud Workstations

Purpose	Tailored for data science and ML with notebooks.	Cloud-based IDE for general software and ML development.
Environment Type	Primarily Jupyter-based notebooks.	Full IDE-based development (e.g., JupyterLab, VS Code, RStudio).
Interface	Notebook UI (Jupyter, JupyterLab).	Full-featured IDE accessible via web browser.
Compute Options	Managed or user-managed notebook VMs with CPU/GPU/TPU.	Highly customizable VMs; ephemeral or persistent dev environments.
Persistence	Notebooks and data persist on attached disk.	File storage is mounted via a persistent volume (separate from VM).
Startup Time	Slower (especially for user-managed notebooks).	Fast startup with pre-configured images.
Customization	Install packages manually in notebooks or via Docker images.	Custom Docker images, workspace templates, project-level configurations.
Collaboration	Via shared notebooks and Git integration.	Git integration, but better support for multiple IDEs and real-time code development workflows.
IAM & Access Control	Per-notebook or VM-based access.	Fine-grained IAM with organization-level workstation configuration.
Typical Users	Data scientists, ML engineers focused on notebooks.	Developers, ML engineers, and teams needing full dev environments.



---

 When to Use Which?

 Use Workbench if:

You prefer Jupyter notebooks for exploration, prototyping, or small ML experiments.

Your workflow revolves around interactive Python coding and data exploration.

You're building and training models directly in notebooks.


 Use Cloud Workstations if:

You want a more traditional development experience (e.g., VS Code or RStudio).

You need to work on larger ML systems, backend services, or codebases.

You want fast, repeatable dev environments with enterprise control and security.

You're collaborating in a team setting where consistent tooling matters.




---

Summary

Think of Workbench as notebook-centric (great for ML experimentation), while Cloud Workstations are full IDEs meant for broader software and ML development workflows  with more flexibility, speed, and team-level control.

Let me know if you want a visual comparison chart or examples of real-world use cases.

