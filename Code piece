
from kfp.v2 import compiler
from kfp.v2.dsl import pipeline, component, Output, Dataset
from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp, DataprocClusterCreateOp, DataprocClusterDeleteOp
from google_cloud_pipeline_components.v1.bigquery import BigQueryQueryJobOp

PROJECT_ID = 'your-project-id'
REGION = 'your-region'
BUCKET_NAME = 'your-gcs-bucket'
CLUSTER_NAME = 'vertex-dataproc-cluster'
PYSPARK_FILE_URI = 'gs://your-bucket/path-to-your-pyspark-job.py'

@pipeline(name="vertex-ai-dataproc-pipeline", description="Pipeline with BigQuery and Dataproc integration")
def dataproc_pipeline():
    # Step 1: Fetch data from BigQuery
    bigquery_query = BigQueryQueryJobOp(
        query="""
            SELECT * FROM `your-project-id.your-dataset.your-table`
            LIMIT 1000
        """,
        project=PROJECT_ID,
        location=REGION,
        output_table=f"{PROJECT_ID}.temp_dataset.temp_table",  # Temporary BQ table for staging
        write_disposition="WRITE_TRUNCATE"
    )
    
    # Step 2: Save BigQuery output to GCS
    bigquery_to_gcs = BigQueryQueryJobOp(
        query=f"""
            EXPORT DATA OPTIONS(
                uri='gs://{BUCKET_NAME}/bigquery_output/*.csv',
                format='CSV',
                overwrite=true
            ) AS
            SELECT * FROM `{PROJECT_ID}.temp_dataset.temp_table`
        """,
        project=PROJECT_ID,
        location=REGION
    ).after(bigquery_query)
    
    # Step 3: Create a Dataproc cluster
    create_cluster = DataprocClusterCreateOp(
        project=PROJECT_ID,
        region=REGION,
        cluster_name=CLUSTER_NAME,
        gcs_bucket=BUCKET_NAME,
        cluster_config={
            "master_config": {"num_instances": 1, "machine_type_uri": "n1-standard-4"},
            "worker_config": {"num_instances": 2, "machine_type_uri": "n1-standard-4"},
        }
    ).after(bigquery_to_gcs)
    
    # Step 4: Submit PySpark job to Dataproc
    pyspark_task = DataprocPySparkBatchOp(
        project=PROJECT_ID,
        location=REGION,
        batch_id="pyspark-job",
        main_python_file_uri=PYSPARK_FILE_URI,
        cluster_name=create_cluster.output["cluster_name"],
        args=[
            f"gs://{BUCKET_NAME}/bigquery_output/*.csv",  # Input data path
            f"gs://{BUCKET_NAME}/processed_output/"       # Output data path
        ]
    ).after(create_cluster)
    
    # Step 5: Delete the cluster after the job is complete
    delete_cluster = DataprocClusterDeleteOp(
        project=PROJECT_ID,
        region=REGION,
        cluster_name=create_cluster.output["cluster_name"]
    ).after(pyspark_task)

compiler.Compiler().compile(
    pipeline_func=dataproc_pipeline,
    package_path="vertex_ai_dataproc_with_bigquery_pipeline.json"
)


from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='your-region')

# Trigger the pipeline
job = aiplatform.PipelineJob(
    display_name="vertex-ai-dataproc-pipeline",
    template_path="vertex_ai_dataproc_with_bigquery_pipeline.json",
    pipeline_root="gs://your-gcs-bucket/pipeline-root/",
    parameter_values={}  # Add any parameters if needed
)

job.run()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize stopwords, lemmatizer, and stemmer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("TextProcessing") \
    .getOrCreate()

# Input and output paths
input_path = "gs://your-bucket/bigquery_output/*.csv"  # Input data fetched from BigQuery
output_path = "gs://your-bucket/processed_output/"    # Processed output path

# Load data
df = spark.read.csv(input_path, header=True, inferSchema=True)

# UDFs for text processing
def clean_html(text):
    """Remove HTML tags from text."""
    return BeautifulSoup(text, "html.parser").get_text()

def clean_urls(text):
    """Remove URLs from text."""
    return re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

def remove_stopwords(text):
    """Remove stopwords from text."""
    words = text.split()
    return " ".join([word for word in words if word.lower() not in stop_words])

def lemmatize(text):
    """Perform lemmatization on text."""
    words = text.split()
    return " ".join([lemmatizer.lemmatize(word) for word in words])

def stem(text):
    """Perform stemming on text."""
    words = text.split()
    return " ".join([stemmer.stem(word) for word in words])

def process_text(text):
    """Perform full text processing: HTML removal, URL removal, stopwords removal, lemmatization, and stemming."""
    if not text:
        return ""
    text = clean_html(text)
    text = clean_urls(text)
    text = remove_stopwords(text)
    text = lemmatize(text)
    text = stem(text)
    return text

# Register UDF
process_text_udf = udf(process_text, StringType())

# Apply text processing
processed_df = df.withColumn("processed_text", process_text_udf(col("text")))

# Save processed data
processed_df.write.csv(output_path, header=True, mode="overwrite")

# Stop Spark Session
spark.stop()



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import tensorflow as tf
import nltk

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize stopwords, lemmatizer, and stemmer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("TextProcessingTFRecords") \
    .getOrCreate()

# Input and output paths
input_path = "gs://your-bucket/tfrecords_data/*.tfrecord"  # TFRecords input path
output_path = "gs://your-bucket/processed_output/"        # Processed output path

# Function to parse TFRecords
def parse_example(serialized_example):
    """Parse a single TFRecord example."""
    feature_description = {
        "text": tf.io.FixedLenFeature([], tf.string),  # Text column in TFRecord
    }
    example = tf.io.parse_single_example(serialized_example, feature_description)
    return {"text": example["text"].numpy().decode("utf-8")}

# Load TFRecords as a Spark DataFrame
raw_rdd = spark.sparkContext.newAPIHadoopFile(
    input_path,
    "org.tensorflow.hadoop.io.TFRecordFileInputFormat",
    "org.apache.hadoop.io.BytesWritable",
    "org.apache.hadoop.io.NullWritable",
).map(lambda x: parse_example(x[0].copyBytes()))

df = spark.createDataFrame(raw_rdd)

# UDFs for text processing
def clean_html(text):
    """Remove HTML tags from text."""
    return BeautifulSoup(text, "html.parser").get_text()

def clean_urls(text):
    """Remove URLs from text."""
    return re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

def remove_stopwords(text):
    """Remove stopwords from text."""
    words = text.split()
    return " ".join([word for word in words if word.lower() not in stop_words])

def lemmatize(text):
    """Perform lemmatization on text."""
    words = text.split()
    return " ".join([lemmatizer.lemmatize(word) for word in words])

def stem(text):
    """Perform stemming on text."""
    words = text.split()
    return " ".join([stemmer.stem(word) for word in words])

def process_text(text):
    """Perform full text processing: HTML removal, URL removal, stopwords removal, lemmatization, and stemming."""
    if not text:
        return ""
    text = clean_html(text)
    text = clean_urls(text)
    text = remove_stopwords(text)
    text = lemmatize(text)
    text = stem(text)
    return text

# Register UDF
process_text_udf = udf(process_text, StringType())

# Apply text processing
processed_df = df.withColumn("processed_text", process_text_udf(col("text")))

# Save processed data
processed_df.write.csv(output_path, header=True, mode="overwrite")

# Stop Spark Session
spark.stop()


from google_cloud_pipeline_components.v1.dataproc.create_pyspark_batch.component import dataproc_create_pyspark_batch as DataprocPySparkBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_batch.component import dataproc_create_spark_batch as DataprocSparkBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_r_batch.component import dataproc_create_spark_r_batch as DataprocSparkRBatchOp
from google_cloud_pipeline_components.v1.dataproc.create_spark_sql_batch.component import dataproc_create_spark_sql_batch as DataprocSparkSqlBatchOp


from google_cloud_pipeline_components.v1.bigquery.create_model.component import bigquery_create_model_job as BigqueryCreateModelJobOp
from google_cloud_pipeline_components.v1.bigquery.detect_anomalies_model.component import bigquery_detect_anomalies_job as BigqueryDetectAnomaliesModelJobOp
from google_cloud_pipeline_components.v1.bigquery.drop_model.component import bigquery_drop_model_job as BigqueryDropModelJobOp
from google_cloud_pipeline_components.v1.bigquery.evaluate_model.component import bigquery_evaluate_model_job as BigqueryEvaluateModelJobOp
from google_cloud_pipeline_components.v1.bigquery.explain_forecast_model.component import bigquery_explain_forecast_model_job as BigqueryExplainForecastModelJobOp
from google_cloud_pipeline_components.v1.bigquery.explain_predict_model.component import bigquery_explain_predict_model_job as BigqueryExplainPredictModelJobOp
from google_cloud_pipeline_components.v1.bigquery.export_model.component import bigquery_export_model_job as BigqueryExportModelJobOp
from google_cloud_pipeline_components.v1.bigquery.feature_importance.component import bigquery_ml_feature_importance_job as BigqueryMLFeatureImportanceJobOp
from google_cloud_pipeline_components.v1.bigquery.forecast_model.component import bigquery_forecast_model_job as BigqueryForecastModelJobOp
from google_cloud_pipeline_components.v1.bigquery.global_explain.component import bigquery_ml_global_explain_job as BigqueryMLGlobalExplainJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_advanced_weights.component import bigquery_ml_advanced_weights_job as BigqueryMLAdvancedWeightsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_arima_coefficients.component import bigquery_ml_arima_coefficients as BigqueryMLArimaCoefficientsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_arima_evaluate.component import bigquery_ml_arima_evaluate_job as BigqueryMLArimaEvaluateJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_centroids.component import bigquery_ml_centroids_job as BigqueryMLCentroidsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_confusion_matrix.component import bigquery_ml_confusion_matrix_job as BigqueryMLConfusionMatrixJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_feature_info.component import bigquery_ml_feature_info_job as BigqueryMLFeatureInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_principal_component_info.component import bigquery_ml_principal_component_info_job as BigqueryMLPrincipalComponentInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_principal_components.component import bigquery_ml_principal_components_job as BigqueryMLPrincipalComponentsJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_recommend.component import bigquery_ml_recommend_job as BigqueryMLRecommendJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_reconstruction_loss.component import bigquery_ml_reconstruction_loss_job as BigqueryMLReconstructionLossJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_roc_curve.component import bigquery_ml_roc_curve_job as BigqueryMLRocCurveJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_training_info.component import bigquery_ml_training_info_job as BigqueryMLTrainingInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_trial_info.component import bigquery_ml_trial_info_job as BigqueryMLTrialInfoJobOp
from google_cloud_pipeline_components.v1.bigquery.ml_weights.component import bigquery_ml_weights_job as BigqueryMLWeightsJobOp
from google_cloud_pipeline_components.v1.bigquery.predict_model.component import bigquery_predict_model_job as BigqueryPredictModelJobOp
from google_cloud_pipeline_components.v1.bigquery.query_job.component import bigquery_query_job as BigqueryQueryJobOp



from kfp.v2.dsl import pipeline
from google_cloud_pipeline_components.v1.dataproc import DataprocSparkSqlBatchOp, DataprocPySparkBatchOp

@pipeline(name="dataproc-spark-sql-nlp-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def spark_sql_nlp_pipeline():
    # Step 1: Create a Dataproc cluster
    create_cluster_task = create_dataproc_cluster(
        project_id="your_project_id",
        region="your_region",  # e.g., "us-central1"
        cluster_name="example-cluster",
    )

    # Step 2: Run a Spark SQL batch job
    spark_sql_task = DataprocSparkSqlBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="spark-sql-batch",
        batch={
            "spark_sql_batch": {
                "query_file_uri": "gs://your-bucket/sql/queries.sql",  # SQL script stored in GCS
                "jar_file_uris": [],  # Optional additional JARs
            },
        },
        cluster_name=create_cluster_task.output,  # Use the created cluster
    )

    # Step 3: Process data with a PySpark job (NLP tasks)
    nlp_task = DataprocPySparkBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="nlp-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/nlp_job.py",  # PySpark script
                "args": [
                    "--input_path", "gs://your-bucket/sql_output/result.csv",  # Output from Step 2
                    "--output_path", "gs://your-bucket/nlp_output/processed_result.csv",
                ],
            },
        },
        cluster_name=create_cluster_task.output,  # Use the created cluster
    )
    nlp_task.after(spark_sql_task)  # Ensure it runs after the SQL job

    # Step 4: Delete the Dataproc cluster after all jobs are completed
    delete_cluster_task = delete_dataproc_cluster(
        project_id="your_project_id",
        region="your_region",
        cluster_name=create_cluster_task.output,
    )
    delete_cluster_task.after(nlp_task)  # Ensure deletion happens after NLP task completes



from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
import re
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder.appName("NLP Tasks").getOrCreate()

# Define helper functions
def remove_html(text):
    return BeautifulSoup(text, "html.parser").get_text()

def remove_urls(text):
    return re.sub(r"http\S+|www.\S+", "", text)

def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    return " ".join([word for word in text.split() if word.lower() not in stop_words])

def stem_text(text):
    stemmer = PorterStemmer()
    return " ".join([stemmer.stem(word) for word in text.split()])

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

# Register UDFs
remove_html_udf = udf(remove_html, StringType())
remove_urls_udf = udf(remove_urls, StringType())
remove_stopwords_udf = udf(remove_stopwords, StringType())
stem_text_udf = udf(stem_text, StringType())
lemmatize_text_udf = udf(lemmatize_text, StringType())

# Input and output paths
input_path = "gs://your-bucket/sql_output/result.csv"  # Replace with the input path
output_path = "gs://your-bucket/nlp_output/processed_result.csv"  # Replace with the output path

# Read input data
df = spark.read.csv(input_path, header=True)

# Apply NLP transformations
df = df.withColumn("text_no_html", remove_html_udf(col("text_column")))
df = df.withColumn("text_no_urls", remove_urls_udf(col("text_no_html")))
df = df.withColumn("text_no_stopwords", remove_stopwords_udf(col("text_no_urls")))
df = df.withColumn("stemmed_text", stem_text_udf(col("text_no_stopwords")))
df = df.withColumn("lemmatized_text", lemmatize_text_udf(col("stemmed_text")))

# Write the result back to GCS
df.write.csv(output_path, header=True)


from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("BigQuery-Spark Integration") \
    .getOrCreate()

# Define configurations
project_id = "your_project_id"
input_table = "your_project_id.your_dataset.your_table"
output_table = "your_project_id.your_dataset.output_table"

# Read data from BigQuery into a Spark DataFrame
df = spark.read.format("bigquery") \
    .option("project", project_id) \
    .option("table", input_table) \
    .load()

# Perform a transformation (e.g., filtering rows)
transformed_df = df.filter(df["column_name"] == "some_value")

# Write the transformed data back to BigQuery
transformed_df.write.format("bigquery") \
    .option("table", output_table) \
    .option("writeMethod", "direct") \  # Options: 'direct' (default) or 'indirect'
    .save()

# Stop the Spark session
spark.stop()


from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp
from kfp.v2.dsl import pipeline

@pipeline(name="bigquery-spark-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def bigquery_spark_pipeline():
    spark_job = DataprocPySparkBatchOp(
        project="your_project_id",
        location="your_region",
        batch_id="bigquery-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/bigquery_spark_job.py",  # Path to PySpark script
            },
        },
        cluster_name="your-dataproc-cluster",  # Use a pre-existing or dynamically created cluster
    )



from kfp.v2.dsl import pipeline, component
from kfp.v2.compiler import Compiler
from google.cloud import dataproc_v1
from google.cloud import aiplatform
import time


@component
def create_dataproc_cluster(project: str, region: str, cluster_name: str):
    """Creates a Dataproc cluster."""
    cluster_client = dataproc_v1.ClusterControllerClient(client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"})
    cluster_config = {
        "project_id": project,
        "config": {
            "gce_cluster_config": {
                "zone_uri": f"{region}-b",  # Change to preferred zone
            },
            "master_config": {
                "num_instances": 1,
                "machine_type_uri": "n1-standard-4",
            },
            "worker_config": {
                "num_instances": 2,
                "machine_type_uri": "n1-standard-4",
            },
        },
    }
    operation = cluster_client.create_cluster(
        request={"project_id": project, "region": region, "cluster": {"cluster_name": cluster_name, **cluster_config}}
    )
    result = operation.result()
    print(f"Cluster created: {result.cluster_name}")


@component
def delete_dataproc_cluster(project: str, region: str, cluster_name: str):
    """Deletes a Dataproc cluster."""
    cluster_client = dataproc_v1.ClusterControllerClient(client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"})
    operation = cluster_client.delete_cluster(
        request={"project_id": project, "region": region, "cluster_name": cluster_name}
    )
    operation.result()
    print(f"Cluster deleted: {cluster_name}")


@pipeline(name="bigquery-spark-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def bigquery_spark_pipeline():
    project = "your_project_id"
    region = "europe-west2"
    cluster_name = f"bigquery-spark-cluster-{int(time.time())}"  # Unique cluster name

    # Step 1: Create a Dataproc cluster
    create_cluster_task = create_dataproc_cluster(project=project, region=region, cluster_name=cluster_name)

    # Step 2: Submit the PySpark job
    pyspark_job_task = DataprocPySparkBatchOp(
        project=project,
        location=region,
        batch_id="bigquery-spark-job",
        batch={
            "pyspark_batch": {
                "main_python_file_uri": "gs://your-bucket/pyspark/bigquery_spark_job.py",
            },
        },
        cluster_name=cluster_name,
    )
    pyspark_job_task.after(create_cluster_task)

    # Step 3: Delete the Dataproc cluster after the job is complete
    delete_cluster_task = delete_dataproc_cluster(project=project, region=region, cluster_name=cluster_name)
    delete_cluster_task.after(pyspark_job_task)


# Compile the pipeline
Compiler().compile(
    pipeline_func=bigquery_spark_pipeline,
    package_path="bigquery_spark_pipeline.json"
)

# Submit the pipeline to Vertex AI
aiplatform.init(project="your_project_id", location="europe-west2")

pipeline_job = aiplatform.PipelineJob(
    display_name="bigquery-spark-pipeline",
    template_path="bigquery_spark_pipeline.json",
)

pipeline_job.run(sync=True)


from google.cloud import dataproc_v1

# Initialize the client with the correct endpoint
region = "europe-west2"
client = dataproc_v1.ClusterControllerClient(
    client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"}
)

# Project and cluster details
project_id = "your_project_id"
cluster_name = "example-cluster"
zone = f"{region}-b"  # Adjust the zone as needed

# Cluster configuration
cluster_config = {
    "project_id": project_id,
    "cluster_name": cluster_name,
    "config": {
        "gce_cluster_config": {
            "zone_uri": zone,
        },
        "master_config": {
            "num_instances": 1,
            "machine_type_uri": "n1-standard-4",
        },
        "worker_config": {
            "num_instances": 2,
            "machine_type_uri": "n1-standard-4",
        },
    },
}

# Create the cluster
operation = client.create_cluster(
    request={"project_id": project_id, "region": region, "cluster": cluster_config}
)
cluster = operation.result()
print(f"Cluster created: {cluster.cluster_name}")

# Delete the cluster
delete_operation = client.delete_cluster(
    request={"project_id": project_id, "region": region, "cluster_name": cluster_name}
)
delete_operation.result()
print(f"Cluster deleted: {cluster_name}")


try:
    client = dataproc_v1.ClusterControllerClient(
        client_options={"api_endpoint": "europe-west2-dataproc.googleapis.com:443"}
    )
    print("Client initialized successfully!")
except Exception as e:
    print(f"Failed to initialize client: {e}")


import subprocess
from kfp.v2.dsl import component

@component
def gcloud_create_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Creates a Dataproc cluster using gcloud CLI."""
    command = [
        "gcloud", "dataproc", "clusters", "create", cluster_name,
        "--project", project_id,
        "--region", region,
        "--zone", f"{region}-b",  # Adjust zone if needed
        "--master-machine-type", "n1-standard-4",
        "--worker-machine-type", "n1-standard-4",
        "--num-workers", "2",
    ]
    subprocess.run(command, check=True)
    print(f"Cluster {cluster_name} created.")

@component
def gcloud_delete_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Deletes a Dataproc cluster using gcloud CLI."""
    command = [
        "gcloud", "dataproc", "clusters", "delete", cluster_name,
        "--project", project_id,
        "--region", region,
        "--quiet",  # Skips confirmation prompt
    ]
    subprocess.run(command, check=True)
    print(f"Cluster {cluster_name} deleted.")



import google.auth
import requests
from kfp.v2.dsl import component

@component
def rest_create_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Creates a Dataproc cluster using REST API."""
    credentials, _ = google.auth.default()
    access_token = credentials.token
    endpoint = f"https://dataproc.googleapis.com/v1/projects/{project_id}/regions/{region}/clusters"

    cluster_config = {
        "clusterName": cluster_name,
        "config": {
            "gceClusterConfig": {"zoneUri": f"{region}-b"},
            "masterConfig": {"numInstances": 1, "machineTypeUri": "n1-standard-4"},
            "workerConfig": {"numInstances": 2, "machineTypeUri": "n1-standard-4"},
        },
    }

    headers = {"Authorization": f"Bearer {access_token}"}
    response = requests.post(endpoint, json=cluster_config, headers=headers)

    if response.status_code == 200:
        print(f"Cluster {cluster_name} created.")
    else:
        print(f"Failed to create cluster: {response.text}")

@component
def rest_delete_dataproc_cluster(project_id: str, region: str, cluster_name: str):
    """Deletes a Dataproc cluster using REST API."""
    credentials, _ = google.auth.default()
    access_token = credentials.token
    endpoint = f"https://dataproc.googleapis.com/v1/projects/{project_id}/regions/{region}/clusters/{cluster_name}"

    headers = {"Authorization": f"Bearer {access_token}"}
    response = requests.delete(endpoint, headers=headers)

    if response.status_code == 200:
        print(f"Cluster {cluster_name} deleted.")
    else:
        print(f"Failed to delete cluster: {response.text}")



from kfp.v2.dsl import pipeline

@pipeline(name="dataproc-cluster-pipeline", pipeline_root="gs://your-bucket/pipeline-root")
def dataproc_pipeline():
    project_id = "your_project_id"
    region = "europe-west2"
    cluster_name = "example-cluster"

    # Step 1: Create Dataproc cluster
    create_task = gcloud_create_dataproc_cluster(
        project_id=project_id,
        region=region,
        cluster_name=cluster_name,
    )

    # Step 2: Delete Dataproc cluster
    delete_task = gcloud_delete_dataproc_cluster(
        project_id=project_id,
        region=region,
        cluster_name=cluster_name,
    )
    delete_task.after(create_task)
 
# Copyright 2023 The Kubeflow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, List

from google_cloud_pipeline_components import _image
from google_cloud_pipeline_components import _placeholders
from kfp.dsl import ConcatPlaceholder
from kfp.dsl import container_component
from kfp.dsl import ContainerSpec
from kfp.dsl import OutputPath


@container_component
def dataproc_create_pyspark_batch(
    main_python_file_uri: str,
    gcp_resources: OutputPath(str),
    location: str = 'us-central1',
    batch_id: str = '',
    labels: Dict[str, str] = {},
    container_image: str = '',
    runtime_config_version: str = '',
    runtime_config_properties: Dict[str, str] = {},
    service_account: str = '',
    network_tags: List[str] = [],
    kms_key: str = '',
    network_uri: str = '',
    subnetwork_uri: str = '',
    metastore_service: str = '',
    spark_history_dataproc_cluster: str = '',
    python_file_uris: List[str] = [],
    jar_file_uris: List[str] = [],
    file_uris: List[str] = [],
    archive_uris: List[str] = [],
    args: List[str] = [],
    project: str = _placeholders.PROJECT_ID_PLACEHOLDER,
):
  # fmt: off
  """Create a Dataproc PySpark batch workload and wait for it to finish.

  Args:
      location: Location of the Dataproc batch workload. If not set, defaults to `"us-central1"`.
      batch_id: The ID to use for the batch, which will become the final component of the batch's resource name. If none is specified, a default name will be generated by the component.  This value must be 4-63 characters. Valid characters are `/[a-z][0-9]-/`.
      labels: The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035. Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035. No more than 32 labels can be associated with a batch.  An object containing a list of `"key": value` pairs. Example: `{ "name": "wrench", "mass": "1.3kg", "count": "3" }`.
      container_image: Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
      runtime_config_version: Version of the batch runtime.
      runtime_config_properties: Runtime configuration for the workload.
      service_account: Service account that is used to execute the workload.
      network_tags: Tags used for network traffic control.
      kms_key: The Cloud KMS key to use for encryption.
      network_uri: Network URI to connect workload to.
      subnetwork_uri: Subnetwork URI to connect workload to.
      metastore_service: Resource name of an existing Dataproc Metastore service.
      spark_history_dataproc_cluster: The Spark History Server configuration for the workload.
      main_python_file_uri: The HCFS URI of the main Python file to use as the Spark driver. Must be a `.py` file.
      python_file_uris: HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: `.py`, `.egg`, and `.zip`.
      jar_file_uris: HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
      file_uris: HCFS URIs of files to be placed in the working directory of each executor.
      archive_uris: HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
      args: The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.
      project: Project to run the Dataproc batch workload. Defaults to the project in which the PipelineJob is run.

  Returns:
      gcp_resources: Serialized gcp_resources proto tracking the Dataproc batch workload. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
  """
  # fmt: on
  return ContainerSpec(
      image=_image.GCPC_IMAGE_TAG,
      command=[
          'python3',
          '-u',
          '-m',
          'google_cloud_pipeline_components.container.v1.dataproc.create_pyspark_batch.launcher',
      ],
      args=[
          '--type',
          'DataprocPySparkBatch',
          '--payload',
          ConcatPlaceholder([
              '{',
              '"labels": ',
              labels,
              ', "runtime_config": {',
              '"version": "',
              runtime_config_version,
              '"',
              ', "container_image": "',
              container_image,
              '"',
              ', "properties": ',
              runtime_config_properties,
              '}',
              ', "environment_config": {',
              '"execution_config": {',
              '"service_account": "',
              service_account,
              '"',
              ', "network_tags": ',
              network_tags,
              ', "kms_key": "',
              kms_key,
              '"',
              ', "network_uri": "',
              network_uri,
              '"',
              ', "subnetwork_uri": "',
              subnetwork_uri,
              '"',
              '}',
              ', "peripherals_config": {',
              '"metastore_service": "',
              metastore_service,
              '"',
              ', "spark_history_server_config": { ',
              '"dataproc_cluster": "',
              spark_history_dataproc_cluster,
              '"',
              '}',
              '}',
              '}',
              ', "pyspark_batch": {',
              '"main_python_file_uri": "',
              main_python_file_uri,
              '"',
              ', "python_file_uris": ',
              python_file_uris,
              ', "jar_file_uris": ',
              jar_file_uris,
              ', "file_uris": ',
              file_uris,
              ', "archive_uris": ',
              archive_uris,
              ', "args": ',
              args,
              '}',
              '}',
          ]),
          '--project',
          project,
          '--location',
          location,
          '--batch_id',
          batch_id,
          '--gcp_resources',
          gcp_resources,
      ],
  )

from pyspark.sql import SparkSession

def save_dataframe_to_gcs():
    # Initialize SparkSession
    spark = SparkSession.builder \
        .appName("SaveDataFrameToParquet") \
        .getOrCreate()

    # Example data to create a DataFrame
    data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
    columns = ["Name", "Age"]
    df = spark.createDataFrame(data, columns)

    # GCS bucket path
    gcs_bucket = "gs://your-gcs-bucket-name/path/to/output"

    # Save DataFrame as Parquet to GCS
    df.write \
        .mode("overwrite") \  # or "append"
        .parquet(gcs_bucket)

    print(f"DataFrame successfully saved to {gcs_bucket}")

if __name__ == "__main__":
    save_dataframe_to_gcs()


from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp

dataproc_pyspark_op = DataprocPySparkBatchOp(
    project="your-gcp-project-id",
    location="your-region",
    batch_id="your-batch-id",
    pyspark_file="gs://your-gcs-bucket-name/path/to/save_to_parquet.py",
    args=[],  # Pass any additional args if needed
    subnetwork_uri="projects/your-project/regions/your-region/subnetworks/your-subnetwork",  # Optional, if needed
    temp_bucket="your-temp-gcs-bucket",
    container_image="gcr.io/your-project-id/pyspark-image:latest",  # Your custom container with PySpark
)

from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("BQ_to_GCS").getOrCreate()

# Define BigQuery and GCS details
bq_table = "your_project.dataset.your_table"
gcs_output_path = "gs://your_bucket/bq_data"

# Load data from BigQuery
df = spark.read.format("bigquery").option("table", bq_table).load()

# Save to GCS in Parquet format
df.write.mode("overwrite").parquet(gcs_output_path)

print(f"Data saved to {gcs_output_path}")
spark.stop()


from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover

# Initialize Spark
spark = SparkSession.builder.appName("NLP_Processing").getOrCreate()

# GCS Input & Output Paths
input_path = "gs://your_bucket/bq_data"
output_path = "gs://your_bucket/nlp_data"

# Load Data
df = spark.read.parquet(input_path)

# NLP Processing: Tokenization & Stopword Removal
tokenizer = Tokenizer(inputCol="text_column", outputCol="words")
df = tokenizer.transform(df)

remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
df = remover.transform(df)

# Save Transformed Data
df.write.mode("overwrite").parquet(output_path)

print(f"NLP Transformed Data saved to {output_path}")
spark.stop()
